{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efe489c-6970-4856-91ea-7a2678dff0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517cca5-1976-49d4-bc3b-2a94fb944a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('genesis')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d635285-a473-4424-b307-b2d66d5a64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"./kurrek.2020.slur-corpus.csv\"\n",
    "data = pd.read_csv(path, sep=\",\", header = 0, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5565270-e765-475e-84b6-89c552e0b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to check if string contains any element from a string\n",
    "# https://bobbyhadz.com/blog/python-check-if-string-contains-element-from-list\n",
    "def is_element_in_string(string, flist):\n",
    "    if any((match := substring) in string for substring in flist):\n",
    "        print('The string contains at least one element from the list')\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5093049e-92f5-4852-b6d0-f06ab37749a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace slur with [REDACTED], and print\n",
    "# Make new data list using pandas\n",
    "# This one is only to print the strings, to avoid printing harmful language. \n",
    "\n",
    "path = \"./kurrek.2020.slur-corpus.csv\"\n",
    "redacted_data = pd.read_csv(path, sep=\",\", header = 0, on_bad_lines='skip')\n",
    "\n",
    "# Iterate for each index in redacted_data, and modify the 'body' to replace the \n",
    "# slur with [REDACTED]. The indexes will remain the same as the one found in the original\n",
    "# document, so this is really to just print out certain rows whilst avoiding printing out the\n",
    "# harmful language (the slurs).\n",
    "for index, row in redacted_data.iterrows():\n",
    "    T1, T2 = str(row['body']), str(row['slur'])\n",
    "    redacted_data.at[index, 'body'] = T1.lower().replace(T2, \"[REDACTED]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b13a32-7cea-4da1-960b-f841ddc776f7",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303123-f3ba-4e6f-9177-b7c8d9aca126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"By constructing a dataframe of posts assigned to the same category, suggest a script that outputs the\n",
    "vocabulary set of each category, the size of the vocabulary, the total number of tokens, the average\n",
    "number of tokens per post and its standard deviation, the average number of pronouns per post and the\n",
    "associated standard deviation, the ten most frequent tokens in each category, excluding the stopword list.\n",
    "Represent the statistical result in a clear table and discuss whether some parameters are most relevant to\n",
    "discriminate a given category.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb95173-a520-4d54-88dd-56335f38f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a list of categories, and the number of frequencies\n",
    "\"\"\"\n",
    "DEG \tDerogatory \t20531\n",
    "NDG \tNon Derogatory Non Appropriative \t16729\n",
    "HOM \tHomonym \t1998\n",
    "APR \tAppropriative \t553\n",
    "CMP \tNoise \t189\n",
    "\"\"\"\n",
    "# Create dictionary variables to store data stored about each category\n",
    "def dict_template():\n",
    "    temp_dict = {\"vocab_set\"   : [],\n",
    "                 \"vocab_size\"  : 0,\n",
    "                 \"total_tokens\": 0,\n",
    "                 \"avg_tokens\"  : {\"num_tokens\": 0, \"standard_dev\": 0},\n",
    "                 \"avg_pronouns\": {\"num_tokens\": 0, \"standard_dev\": 0},\n",
    "                 \"freq_words\"  : []\n",
    "           }\n",
    "    return temp_dict\n",
    "\n",
    "deg_dict = dict_template()\n",
    "ndg_dict = dict_template()\n",
    "hom_dict = dict_template()\n",
    "apr_dict = dict_template()\n",
    "cmp_dict = dict_template()\n",
    "\n",
    "categ_dict = {0: deg_dict,\n",
    "              1: ndg_dict,\n",
    "              2: hom_dict,\n",
    "              3: apr_dict,\n",
    "              4: cmp_dict\n",
    "             }\n",
    "categ_string_list = {0: \"DEG\",\n",
    "                     1: \"NDG\",\n",
    "                     2: \"HOM\",\n",
    "                     3: \"APR\",\n",
    "                     4: \"CMP\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5f0b04e-eee1-4bfd-8f5c-d8897cf8e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the vocabulary set per category \n",
    "# Flatten out list of lists for vocab set\n",
    "# https://stackoverflow.com/questions/952914/how-do-i-make-a-flat-list-out-of-a-list-of-lists\n",
    "# https://stackoverflow.com/questions/10677020/real-word-count-in-nltk\n",
    "# https://www.geeksforgeeks.org/python-statistics-stdev/\n",
    "# https://stackoverflow.com/questions/35086440/python-how-to-compute-the-top-x-most-frequently-used-words-in-an-nltk-corpus\n",
    "import itertools\n",
    "import statistics\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def find_vocab_set(categ):\n",
    "    # Iterate each index\n",
    "    tokenized_bodies = []\n",
    "    total_tokens = 0\n",
    "    total_bodies = 0\n",
    "    size_of_doc = []\n",
    "    pronouns_num_list = []\n",
    "    total_pronouns = 0\n",
    "    most_frequent_words = []\n",
    "    for index, row in data.iterrows():\n",
    "        # Create body and label integer and string\n",
    "        body, label = str(row['body']), str(row['gold_label'])\n",
    "        # Check if the label is deg, ndg, hom, apr, or cmp\n",
    "        if label == categ_string_list[categ]:\n",
    "            tokenized_body = word_tokenize(body)\n",
    "            # Print tokenized body\n",
    "            tokenized_bodies.extend(tokenized_body)\n",
    "            # Update total tokens\n",
    "            total_tokens += len(tokenized_body)\n",
    "            # Update size of docs list\n",
    "            size_of_doc.append(len(tokenized_body))\n",
    "            # Update pronounds list\n",
    "            tmp_pronoun = extract_pronouns(body)\n",
    "            pronouns_num_list.append(tmp_pronoun)\n",
    "            # Update total pronounds count\n",
    "            total_pronouns += tmp_pronoun\n",
    "            # Update total bodies\n",
    "            total_bodies += 1\n",
    "    # Remove doubles\n",
    "    categ_dict[categ][\"vocab_set\"] = list(set(tokenized_bodies))\n",
    "    # Update vocab size\n",
    "    categ_dict[categ][\"vocab_size\"] = len(categ_dict[categ][\"vocab_set\"])\n",
    "    # Update total number of tokens\n",
    "    categ_dict[categ][\"total_tokens\"] = total_tokens\n",
    "    # Update avg tokens:\n",
    "    categ_dict[categ][\"avg_tokens\"][\"num_tokens\"] = total_tokens / total_bodies\n",
    "    # Update token standard deviation\n",
    "    categ_dict[categ][\"avg_tokens\"][\"standard_dev\"] = statistics.stdev(size_of_doc)\n",
    "    # Update avg pronouns\n",
    "    categ_dict[categ][\"avg_pronouns\"][\"num_tokens\"] = total_pronouns / total_bodies\n",
    "    # Update pronoun standard deviation\n",
    "    categ_dict[categ][\"avg_pronouns\"][\"standard_dev\"] = statistics.stdev(pronouns_num_list)\n",
    "    # Remove stopwords\n",
    "    Stopwords = set(upper_word.upper() for upper_word in nltk.corpus.stopwords.words('english'))\n",
    "    Stopwords.add(\"HTTP\")\n",
    "    Stopwords.add(\"HTTPS\")\n",
    "    bodies_without_stopwords = [word.upper() for word in tokenized_bodies if word.isalpha() and word.upper() not in Stopwords]\n",
    "    # Update frequent tokens, without stopwords\n",
    "    most_frequent_words = FreqDist(bodies_without_stopwords)\n",
    "    categ_dict[categ][\"freq_words\"] = most_frequent_words.most_common(10)\n",
    "\n",
    "# Function to extract number of pronouns in a given string\n",
    "def extract_pronouns(string):\n",
    "    # Extract pronouns using pos_tag\n",
    "    pos_list = nltk.pos_tag(word_tokenize(string))\n",
    "    number_of_pronouns = 0\n",
    "    # Run through every entry\n",
    "    for pos in pos_list:\n",
    "        if pos[1] == \"PRP\":\n",
    "            number_of_pronouns += 1\n",
    "    return number_of_pronouns\n",
    "            \n",
    "        \n",
    "find_vocab_set(0)\n",
    "find_vocab_set(1)\n",
    "find_vocab_set(2)\n",
    "find_vocab_set(3)\n",
    "find_vocab_set(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ead7d849-6892-4359-9ed2-26ed62d3ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info regarding category    DEG:\n",
      "Vocab size:                36315\n",
      "Total size:                659069\n",
      "Average tokens:            32.10272771553824\n",
      "Token standard dev:        24.983106326025545\n",
      "Average pronouns:          2.314710180224062\n",
      "Pronoun standard dev:      2.467200766115543\n",
      "Ten frequent words:        [('FAGGOT', 7274), ('TRANNY', 6420), ('NIGGER', 5322), ('LIKE', 3727), ('PEOPLE', 2152), ('FUCKING', 2017), ('GET', 1845), ('WOULD', 1765), ('FUCK', 1731), ('TRANNIES', 1710)]\n",
      "\n",
      "\n",
      "Info regarding category    NDG:\n",
      "Vocab size:                31418\n",
      "Total size:                804166\n",
      "Average tokens:            48.07592515095355\n",
      "Token standard dev:        29.349139174586835\n",
      "Average pronouns:          3.803132659771627\n",
      "Pronoun standard dev:      3.206033523455913\n",
      "Ten frequent words:        [('NIGGER', 7541), ('PEOPLE', 5926), ('FAGGOT', 4702), ('LIKE', 4247), ('WORD', 3892), ('TRANNY', 2936), ('BLACK', 2758), ('SAY', 2635), ('WOULD', 2218), ('CALLED', 2208)]\n",
      "\n",
      "\n",
      "Info regarding category    HOM:\n",
      "Vocab size:                10555\n",
      "Total size:                112531\n",
      "Average tokens:            56.32182182182182\n",
      "Token standard dev:        37.924615343569684\n",
      "Average pronouns:          3.7387387387387387\n",
      "Pronoun standard dev:      3.5363214170317785\n",
      "Ten frequent words:        [('TRANNY', 2036), ('CAR', 561), ('LIKE', 462), ('ENGINE', 348), ('GET', 336), ('WOULD', 322), ('TRANSMISSION', 309), ('FLUID', 297), ('ONE', 272), ('NEW', 259)]\n",
      "\n",
      "\n",
      "Info regarding category    APR:\n",
      "Vocab size:                3906\n",
      "Total size:                23333\n",
      "Average tokens:            42.19349005424955\n",
      "Token standard dev:        28.481442488741404\n",
      "Average pronouns:          4.023508137432188\n",
      "Pronoun standard dev:      3.4731616955491775\n",
      "Ten frequent words:        [('TRANNY', 310), ('FAGGOT', 217), ('LIKE', 124), ('PEOPLE', 87), ('GET', 81), ('US', 69), ('ONE', 62), ('TRANNIES', 61), ('TRANS', 57), ('KNOW', 54)]\n",
      "\n",
      "\n",
      "Info regarding category    CMP:\n",
      "Vocab size:                2052\n",
      "Total size:                5604\n",
      "Average tokens:            29.650793650793652\n",
      "Token standard dev:        22.844170584328882\n",
      "Average pronouns:          1.164021164021164\n",
      "Pronoun standard dev:      1.3327210684754003\n",
      "Ten frequent words:        [('NIGGER', 27), ('ONE', 14), ('MAN', 13), ('DET', 13), ('AMP', 13), ('DE', 12), ('POST', 12), ('GT', 11), ('QUE', 11), ('NEGER', 11)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in categ_dict.items():\n",
    "    print(f\"Info regarding category    {categ_string_list[key]}:\")\n",
    "    print(f\"Vocab size:                {value[\"vocab_size\"]}\")\n",
    "    print(f\"Total size:                {value[\"total_tokens\"]}\")\n",
    "    print(f\"Average tokens:            {value[\"avg_tokens\"][\"num_tokens\"]}\")\n",
    "    print(f\"Token standard dev:        {value[\"avg_tokens\"][\"standard_dev\"]}\")\n",
    "    print(f\"Average pronouns:          {value[\"avg_pronouns\"][\"num_tokens\"]}\")\n",
    "    print(f\"Pronoun standard dev:      {value[\"avg_pronouns\"][\"standard_dev\"]}\")\n",
    "    print(f\"Ten frequent words:        {value[\"freq_words\"]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7268e958-df55-44aa-94d8-295431393503",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Nikola Tesla was a Serbian-American engineer, futurist, and inventor. He is known for his contributions to the design of the modern alternating current electricity supply system. Born and raised in the Austrian Empire, Tesla first studied engineering and physics in the 1870s without receiving a degree.\"\n",
    "Stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "Stopwords.extend([\"HTTP\"])\n",
    "words = word_tokenize(sentence)\n",
    "words = [word.lower() for word in words if word.isalpha() and word not in Stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ad151c0-b3ab-40ed-9846-95cfd650d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nikola', 'tesla', 'engineer', 'futurist', 'inventor', 'he', 'known', 'contributions', 'design', 'modern', 'alternating', 'current', 'electricity', 'supply', 'system', 'born', 'raised', 'austrian', 'empire', 'tesla', 'first', 'studied', 'engineering', 'physics', 'without', 'receiving', 'degree']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa631bd-dd54-4bc8-8c2c-693d78fc5798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
