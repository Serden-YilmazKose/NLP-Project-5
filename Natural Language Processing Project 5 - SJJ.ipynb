{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efe489c-6970-4856-91ea-7a2678dff0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517cca5-1976-49d4-bc3b-2a94fb944a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('genesis')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d635285-a473-4424-b307-b2d66d5a64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"./kurrek.2020.slur-corpus.csv\"\n",
    "data = pd.read_csv(path, sep=\",\", header = 0, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5565270-e765-475e-84b6-89c552e0b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to check if string contains any element from a string\n",
    "# https://bobbyhadz.com/blog/python-check-if-string-contains-element-from-list\n",
    "def is_element_in_string(string, flist):\n",
    "    if any((match := substring) in string for substring in flist):\n",
    "        print('The string contains at least one element from the list')\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5093049e-92f5-4852-b6d0-f06ab37749a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace slur with [REDACTED], and print\n",
    "# Make new data list using pandas\n",
    "# This one is only to print the strings, to avoid printing harmful language. \n",
    "\n",
    "path = \"./kurrek.2020.slur-corpus.csv\"\n",
    "redacted_data = pd.read_csv(path, sep=\",\", header = 0, on_bad_lines='skip')\n",
    "\n",
    "# Iterate for each index in redacted_data, and modify the 'body' to replace the \n",
    "# slur with [REDACTED]. The indexes will remain the same as the one found in the original\n",
    "# document, so this is really to just print out certain rows whilst avoiding printing out the\n",
    "# harmful language (the slurs).\n",
    "for index, row in redacted_data.iterrows():\n",
    "    T1, T2 = str(row['body']), str(row['slur'])\n",
    "    redacted_data.at[index, 'body'] = T1.lower().replace(T2, \"[REDACTED]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b13a32-7cea-4da1-960b-f841ddc776f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303123-f3ba-4e6f-9177-b7c8d9aca126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"By constructing a dataframe of posts assigned to the same category, suggest a script that outputs the\n",
    "vocabulary set of each category, the size of the vocabulary, the total number of tokens, the average\n",
    "number of tokens per post and its standard deviation, the average number of pronouns per post and the\n",
    "associated standard deviation, the ten most frequent tokens in each category, excluding the stopword list.\n",
    "Represent the statistical result in a clear table and discuss whether some parameters are most relevant to\n",
    "discriminate a given category.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fcb95173-a520-4d54-88dd-56335f38f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a list of categories, and the number of frequencies\n",
    "\"\"\"\n",
    "DEG \tDerogatory \t20531\n",
    "NDG \tNon Derogatory Non Appropriative \t16729\n",
    "HOM \tHomonym \t1998\n",
    "APR \tAppropriative \t553\n",
    "CMP \tNoise \t189\n",
    "\"\"\"\n",
    "# Create dictionary variables to store data stored about each category\n",
    "def dict_template():\n",
    "    temp_dict = {\"vocab_set\"     : [],\n",
    "                 \"vocab_size\"    : 0,\n",
    "                 \"total_tokens\"  : 0,\n",
    "                 \"avg_tokens\"    : 0,\n",
    "                 \"tokens_standev\": 0,\n",
    "                 \"avg_pronouns\"  : 0,\n",
    "                 \"prp_standev\"   : 0,\n",
    "                 \"freq_words\"    : []\n",
    "           }\n",
    "    return temp_dict\n",
    "\n",
    "deg_dict = dict_template()\n",
    "ndg_dict = dict_template()\n",
    "hom_dict = dict_template()\n",
    "apr_dict = dict_template()\n",
    "cmp_dict = dict_template()\n",
    "\n",
    "categ_dict = {0: deg_dict,\n",
    "              1: ndg_dict,\n",
    "              2: hom_dict,\n",
    "              3: apr_dict,\n",
    "              4: cmp_dict\n",
    "             }\n",
    "categ_string_list = {0: \"DEG\",\n",
    "                     1: \"NDG\",\n",
    "                     2: \"HOM\",\n",
    "                     3: \"APR\",\n",
    "                     4: \"CMP\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5f0b04e-eee1-4bfd-8f5c-d8897cf8e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the vocabulary set per category \n",
    "# Flatten out list of lists for vocab set\n",
    "# https://stackoverflow.com/questions/952914/how-do-i-make-a-flat-list-out-of-a-list-of-lists\n",
    "# https://stackoverflow.com/questions/10677020/real-word-count-in-nltk\n",
    "# https://www.geeksforgeeks.org/python-statistics-stdev/\n",
    "# https://stackoverflow.com/questions/35086440/python-how-to-compute-the-top-x-most-frequently-used-words-in-an-nltk-corpus\n",
    "import itertools\n",
    "import statistics\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def find_vocab_set(categ):\n",
    "    # Iterate each index\n",
    "    tokenized_bodies = []\n",
    "    total_tokens = 0\n",
    "    total_bodies = 0\n",
    "    size_of_doc = []\n",
    "    pronouns_num_list = []\n",
    "    total_pronouns = 0\n",
    "    most_frequent_words = []\n",
    "    for index, row in data.iterrows():\n",
    "        # Create body and label integer and string\n",
    "        body, label = str(row['body']), str(row['gold_label'])\n",
    "        # Check if the label is deg, ndg, hom, apr, or cmp\n",
    "        if label == categ_string_list[categ]:\n",
    "            tokenized_body = word_tokenize(body)\n",
    "            # Print tokenized body\n",
    "            tokenized_bodies.extend(tokenized_body)\n",
    "            # Update total tokens\n",
    "            total_tokens += len(tokenized_body)\n",
    "            # Update size of docs list\n",
    "            size_of_doc.append(len(tokenized_body))\n",
    "            # Update pronounds list\n",
    "            tmp_pronoun = extract_pronouns(body)\n",
    "            pronouns_num_list.append(tmp_pronoun)\n",
    "            # Update total pronounds count\n",
    "            total_pronouns += tmp_pronoun\n",
    "            # Update total bodies\n",
    "            total_bodies += 1\n",
    "    # Remove doubles\n",
    "    categ_dict[categ][\"vocab_set\"] = list(set(tokenized_bodies))\n",
    "    # Update vocab size\n",
    "    categ_dict[categ][\"vocab_size\"] = len(categ_dict[categ][\"vocab_set\"])\n",
    "    # Update total number of tokens\n",
    "    categ_dict[categ][\"total_tokens\"] = total_tokens\n",
    "    # Update avg tokens:\n",
    "    categ_dict[categ][\"avg_tokens\"] = total_tokens / total_bodies\n",
    "    # Update token standard deviation\n",
    "    categ_dict[categ][\"tokens_standev\"] = statistics.stdev(size_of_doc)\n",
    "    # Update avg pronouns\n",
    "    categ_dict[categ][\"avg_pronouns\"] = total_pronouns / total_bodies\n",
    "    # Update pronoun standard deviation\n",
    "    categ_dict[categ][\"prp_standev\"] = statistics.stdev(pronouns_num_list)\n",
    "    # Remove stopwords\n",
    "    Stopwords = set(upper_word.upper() for upper_word in nltk.corpus.stopwords.words('english'))\n",
    "    Stopwords.add(\"HTTP\")\n",
    "    Stopwords.add(\"HTTPS\")\n",
    "    bodies_without_stopwords = [word.upper() for word in tokenized_bodies if word.isalpha() and word.upper() not in Stopwords]\n",
    "    # Update frequent tokens, without stopwords\n",
    "    most_frequent_words = FreqDist(bodies_without_stopwords)\n",
    "    categ_dict[categ][\"freq_words\"] = most_frequent_words.most_common(10)\n",
    "\n",
    "# Function to extract number of pronouns in a given string\n",
    "def extract_pronouns(string):\n",
    "    # Extract pronouns using pos_tag\n",
    "    pos_list = nltk.pos_tag(word_tokenize(string))\n",
    "    number_of_pronouns = 0\n",
    "    # Run through every entry\n",
    "    for pos in pos_list:\n",
    "        if pos[1] == \"PRP\":\n",
    "            number_of_pronouns += 1\n",
    "    return number_of_pronouns\n",
    "            \n",
    "        \n",
    "find_vocab_set(0)\n",
    "find_vocab_set(1)\n",
    "find_vocab_set(2)\n",
    "find_vocab_set(3)\n",
    "find_vocab_set(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ead7d849-6892-4359-9ed2-26ed62d3ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          DEG            NDG            HOM           APR  \\\n",
      "vocab_size       36315.000000   31418.000000   10555.000000   3906.000000   \n",
      "total_tokens    659069.000000  804166.000000  112531.000000  23333.000000   \n",
      "avg_tokens          32.102728      48.075925      56.321822     42.193490   \n",
      "tokens_standev      24.983106      29.349139      37.924615     28.481442   \n",
      "avg_pronouns         2.314710       3.803133       3.738739      4.023508   \n",
      "prp_standev          2.467201       3.206034       3.536321      3.473162   \n",
      "\n",
      "                        CMP  \n",
      "vocab_size      2052.000000  \n",
      "total_tokens    5604.000000  \n",
      "avg_tokens        29.650794  \n",
      "tokens_standev    22.844171  \n",
      "avg_pronouns       1.164021  \n",
      "prp_standev        1.332721  \n"
     ]
    }
   ],
   "source": [
    "# Print DataFrame without array values (Since they have more than one element)\n",
    "# https://stackoverflow.com/questions/31433989/return-copy-of-dictionary-excluding-specified-keys\n",
    "\n",
    "DataFrame_dict = {}\n",
    "\n",
    "def without_keys(d, keys):\n",
    "    return {k: v for k, v in d.items() if k not in keys}\n",
    "\n",
    "for key, value in categ_dict.items():\n",
    "    \"\"\"\n",
    "    print(f\"Info regarding category    {categ_string_list[key]}:\")\n",
    "    print(f\"Vocab size:                {value[\"vocab_size\"]}\")\n",
    "    print(f\"Total size:                {value[\"total_tokens\"]}\")\n",
    "    print(f\"Average tokens:            {value[\"avg_tokens\"][\"num_tokens\"]:.2f}\")\n",
    "    print(f\"Token standard dev:        {value[\"avg_tokens\"][\"standard_dev\"]:.2f}\")\n",
    "    print(f\"Average pronouns:          {value[\"avg_pronouns\"][\"num_tokens\"]:.2f}\")\n",
    "    print(f\"Pronoun standard dev:      {value[\"avg_pronouns\"][\"standard_dev\"]:.2f}\")\n",
    "    print(f\"Ten frequent words:        {value[\"freq_words\"]}\")\n",
    "    print(\"\\n\")\n",
    "    \"\"\"\n",
    "    DataFrame_dict[categ_string_list[key]] = without_keys(value, [\"vocab_set\", \"freq_words\"])\n",
    "# Print dataframe using Pandas\n",
    "print(pd.DataFrame(DataFrame_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bc7406a8-af4f-485b-ade5-05391c6fe3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"WE have gathered lots of data, and can notice interesting aspects of our findings. First of all, the DEG category\\nhas the most words by far, followed, by NDG, HOM, APR, and finally CMP. However, HOM and NDG have more tokens per\\ndocument than the other categories. The lowest standard deviation for the tokens was found in the CMP and DEG \\ncategory. The most pronouns per token were found in APR and NDG. It's respective lowest standardized deviation was \\nfound in the CMP and DEG categories.\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"WE have gathered lots of data, and can notice interesting aspects of our findings. First of all, the DEG category\n",
    "has the most words by far, followed, by NDG, HOM, APR, and finally CMP. However, HOM and NDG have more tokens per\n",
    "document than the other categories. The lowest standard deviation for the tokens was found in the CMP and DEG \n",
    "category. The most pronouns per token were found in APR and NDG. It's respective lowest standardized deviation was \n",
    "found in the CMP and DEG categories.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc3f21-a66d-4fd4-ab4c-b1eb3a5e8fb0",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d97f92-8168-405b-a43d-6371f36fd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Suggest a script to draw and evaluate the zipf’s law fitting for the dataframe of each category, and\n",
    "computing the corresponding R2 and adjusted R2 statistics.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342575ca-689e-49db-92bd-293350849f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
