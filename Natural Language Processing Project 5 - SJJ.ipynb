{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efe489c-6970-4856-91ea-7a2678dff0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9517cca5-1976-49d4-bc3b-2a94fb944a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ssuo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ssuo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ssuo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to /home/ssuo/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/ssuo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/ssuo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ssuo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ssuo/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('genesis')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d635285-a473-4424-b307-b2d66d5a64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"./kurrek.2020.slur-corpus.csv\"\n",
    "data = pd.read_csv(path, sep=\",\", header = 0, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5565270-e765-475e-84b6-89c552e0b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to check if string contains any element from a string\n",
    "# https://bobbyhadz.com/blog/python-check-if-string-contains-element-from-list\n",
    "def is_element_in_string(string, flist):\n",
    "    if any((match := substring) in string for substring in flist):\n",
    "        print('The string contains at least one element from the list')\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5093049e-92f5-4852-b6d0-f06ab37749a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace slur with [REDACTED], and print\n",
    "# Make new data list using pandas\n",
    "# This one is only to print the strings, to avoid printing harmful language. \n",
    "\n",
    "path = \"./kurrek.2020.slur-corpus.csv\"\n",
    "redacted_data = pd.read_csv(path, sep=\",\", header = 0, on_bad_lines='skip')\n",
    "\n",
    "# Iterate for each index in redacted_data, and modify the 'body' to replace the \n",
    "# slur with [REDACTED]. The indexes will remain the same as the one found in the original\n",
    "# document, so this is really to just print out certain rows whilst avoiding printing out the\n",
    "# harmful language (the slurs).\n",
    "for index, row in redacted_data.iterrows():\n",
    "    T1, T2 = str(row['body']), str(row['slur'])\n",
    "    redacted_data.at[index, 'body'] = T1.lower().replace(T2, \"[REDACTED]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b13a32-7cea-4da1-960b-f841ddc776f7",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303123-f3ba-4e6f-9177-b7c8d9aca126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"By constructing a dataframe of posts assigned to the same category, suggest a script that outputs the\n",
    "vocabulary set of each category, the size of the vocabulary, the total number of tokens, the average\n",
    "number of tokens per post and its standard deviation, the average number of pronouns per post and the\n",
    "associated standard deviation, the ten most frequent tokens in each category, excluding the stopword list.\n",
    "Represent the statistical result in a clear table and discuss whether some parameters are most relevant to\n",
    "discriminate a given category.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fcb95173-a520-4d54-88dd-56335f38f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a list of categories, and the number of frequencies\n",
    "\"\"\"\n",
    "DEG \tDerogatory \t20531\n",
    "NDG \tNon Derogatory Non Appropriative \t16729\n",
    "HOM \tHomonym \t1998\n",
    "APR \tAppropriative \t553\n",
    "CMP \tNoise \t189\n",
    "\"\"\"\n",
    "# Create dictionary variables to store data stored about each category\n",
    "def dict_template():\n",
    "    temp_dict = {\"vocab_set\"   : [],\n",
    "                 \"vocab_size\"  : 0,\n",
    "                 \"total_tokens\": 0,\n",
    "                 \"avg_tokens\"  : {\"num_tokens\": 0, \"standard_dev\": 0},\n",
    "                 \"avg_pronouns\": {\"num_tokens\": 0, \"standard_dev\": 0},\n",
    "                 \"freq_words\"  : []\n",
    "           }\n",
    "    return temp_dict\n",
    "\n",
    "deg_dict = dict_template()\n",
    "ndg_dict = dict_template()\n",
    "hom_dict = dict_template()\n",
    "apr_dict = dict_template()\n",
    "cmp_dict = dict_template()\n",
    "\n",
    "categ_dict = {0: deg_dict,\n",
    "              1: ndg_dict,\n",
    "              2: hom_dict,\n",
    "              3: apr_dict,\n",
    "              4: cmp_dict\n",
    "             }\n",
    "categ_string_list = {0: \"DEG\",\n",
    "                     1: \"NDG\",\n",
    "                     2: \"HOM\",\n",
    "                     3: \"APR\",\n",
    "                     4: \"CMP\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d5f0b04e-eee1-4bfd-8f5c-d8897cf8e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the vocabulary set per category \n",
    "# Flatten out list of lists for vocab set\n",
    "# https://stackoverflow.com/questions/952914/how-do-i-make-a-flat-list-out-of-a-list-of-lists\n",
    "# https://stackoverflow.com/questions/10677020/real-word-count-in-nltk\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def find_vocab_set(categ):\n",
    "    # Iterate each index\n",
    "    total_tokens = 0\n",
    "    total_bodies = 0\n",
    "    for index, row in data.iterrows():\n",
    "        # Create body and label integer and string\n",
    "        body, label = str(row['body']), str(row['gold_label'])\n",
    "        # Check if the label is deg, ndg, hom, apr, or cmp\n",
    "        if label == categ_string_list[categ]:\n",
    "            # Print tokenized body\n",
    "            categ_dict[categ][\"vocab_set\"].extend(word_tokenize(body))\n",
    "            # Update total tokens\n",
    "            total_tokens += len(word_tokenize(body))\n",
    "            # Update total bodies\n",
    "            total_bodies += 1\n",
    "    # Remove doubles\n",
    "    categ_dict[categ][\"vocab_set\"] = list(set(categ_dict[categ][\"vocab_set\"]))\n",
    "    # Update vocab size\n",
    "    categ_dict[categ][\"vocab_size\"] = len(categ_dict[categ][\"vocab_set\"])\n",
    "    # Update total number of tokens\n",
    "    categ_dict[categ][\"total_tokens\"] = total_tokens\n",
    "    # Update avg tokens:\n",
    "    categ_dict[categ][\"avg_tokens\"][\"num_tokens\"] = total_tokens / total_bodies\n",
    "        \n",
    "find_vocab_set(0)\n",
    "find_vocab_set(1)\n",
    "find_vocab_set(2)\n",
    "find_vocab_set(3)\n",
    "find_vocab_set(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ead7d849-6892-4359-9ed2-26ed62d3ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info regarding the DEG category:\n",
      "Vocab size:     36315\n",
      "Total size:     659069\n",
      "Average tokens: 32.10272771553824\n",
      "\n",
      "Info regarding the NDG category:\n",
      "Vocab size:     31418\n",
      "Total size:     804166\n",
      "Average tokens: 48.07592515095355\n",
      "\n",
      "Info regarding the HOM category:\n",
      "Vocab size:     10555\n",
      "Total size:     112531\n",
      "Average tokens: 56.32182182182182\n",
      "\n",
      "Info regarding the APR category:\n",
      "Vocab size:     3906\n",
      "Total size:     23333\n",
      "Average tokens: 42.19349005424955\n",
      "\n",
      "Info regarding the CMP category:\n",
      "Vocab size:     2052\n",
      "Total size:     5604\n",
      "Average tokens: 29.650793650793652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in categ_dict.items():\n",
    "    print(f\"Info regarding the {categ_string_list[key]} category:\")\n",
    "    print(f\"Vocab size:     {value[\"vocab_size\"]}\")\n",
    "    print(f\"Total size:     {value[\"total_tokens\"]}\")\n",
    "    print(f\"Average tokens: {value[\"avg_tokens\"][\"num_tokens\"]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a9fac-ca26-4170-96da-bd83b550794f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
