{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740d60ce-eac4-4f09-a495-dfd69ebd39c4",
   "metadata": {},
   "source": [
    "### Code done by Serden-Yilmaz Kose, Jesper Nyman and Jussi Saariniemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efe489c-6970-4856-91ea-7a2678dff0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9517cca5-1976-49d4-bc3b-2a94fb944a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('stopwords')\\nnltk.download('punkt')\\nnltk.download('wordnet')\\nnltk.download('genesis')\\n\\nnltk.download('punkt_tab')\\nnltk.download('averaged_perceptron_tagger_eng')\\nnltk.download('averaged_perceptron_tagger')\\nnltk.download('vader_lexicon')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('genesis')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d635285-a473-4424-b307-b2d66d5a64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"./kurrek.2020.slur-corpus.csv\"\n",
    "data = pd.read_csv(path, sep=\",\", header = 0, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5565270-e765-475e-84b6-89c552e0b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to check if string contains any element from a string\n",
    "# https://bobbyhadz.com/blog/python-check-if-string-contains-element-from-list\n",
    "def is_element_in_string(string, flist):\n",
    "    if any((match := substring) in string for substring in flist):\n",
    "        print('The string contains at least one element from the list')\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5093049e-92f5-4852-b6d0-f06ab37749a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace slur with [REDACTED], and print\n",
    "# Make new data list using pandas\n",
    "# This one is only to print the strings, to avoid printing harmful language. \n",
    "\n",
    "path = \"./kurrek.2020.slur-corpus.csv\"\n",
    "redacted_data = pd.read_csv(path, sep=\",\", header = 0, on_bad_lines='skip')\n",
    "\n",
    "# Iterate for each index in redacted_data, and modify the 'body' to replace the \n",
    "# slur with [REDACTED]. The indexes will remain the same as the one found in the original\n",
    "# document, so this is really to just print out certain rows whilst avoiding printing out the\n",
    "# harmful language (the slurs).\n",
    "for index, row in redacted_data.iterrows():\n",
    "    T1, T2 = str(row['body']), str(row['slur'])\n",
    "    redacted_data.at[index, 'body'] = T1.lower().replace(T2, \"[REDACTED]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b13a32-7cea-4da1-960b-f841ddc776f7",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8303123-f3ba-4e6f-9177-b7c8d9aca126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By constructing a dataframe of posts assigned to the same category, suggest a script that outputs the\\nvocabulary set of each category, the size of the vocabulary, the total number of tokens, the average\\nnumber of tokens per post and its standard deviation, the average number of pronouns per post and the\\nassociated standard deviation, the ten most frequent tokens in each category, excluding the stopword list.\\nRepresent the statistical result in a clear table and discuss whether some parameters are most relevant to\\ndiscriminate a given category.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"By constructing a dataframe of posts assigned to the same category, suggest a script that outputs the\n",
    "vocabulary set of each category, the size of the vocabulary, the total number of tokens, the average\n",
    "number of tokens per post and its standard deviation, the average number of pronouns per post and the\n",
    "associated standard deviation, the ten most frequent tokens in each category, excluding the stopword list.\n",
    "Represent the statistical result in a clear table and discuss whether some parameters are most relevant to\n",
    "discriminate a given category.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb95173-a520-4d54-88dd-56335f38f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a list of categories, and the number of frequencies\n",
    "\"\"\"\n",
    "DEG \tDerogatory \t20531\n",
    "NDG \tNon Derogatory Non Appropriative \t16729\n",
    "HOM \tHomonym \t1998\n",
    "APR \tAppropriative \t553\n",
    "CMP \tNoise \t189\n",
    "\"\"\"\n",
    "# Create dictionary variables to store data stored about each category\n",
    "def dict_template():\n",
    "    temp_dict = {\"vocab_set\"     : [],\n",
    "                 \"vocab_size\"    : 0,\n",
    "                 \"total_tokens\"  : 0,\n",
    "                 \"avg_tokens\"    : 0,\n",
    "                 \"tokens_standev\": 0,\n",
    "                 \"avg_pronouns\"  : 0,\n",
    "                 \"prp_standev\"   : 0,\n",
    "                 \"freq_words\"    : [],\n",
    "                 \"all_words\"     : [],\n",
    "                 \"entire_words\"  : [] \n",
    "           }\n",
    "    return temp_dict\n",
    "\n",
    "deg_dict = dict_template()\n",
    "ndg_dict = dict_template()\n",
    "hom_dict = dict_template()\n",
    "apr_dict = dict_template()\n",
    "cmp_dict = dict_template()\n",
    "\n",
    "# Make category dictionary to store dictionaries by index\n",
    "categ_dict = {0: deg_dict,\n",
    "              1: ndg_dict,\n",
    "              2: hom_dict,\n",
    "              3: apr_dict,\n",
    "              4: cmp_dict\n",
    "             }\n",
    "\n",
    "# Store names of categories by index in this dictionary\n",
    "categ_string_list = {0: \"DEG\",\n",
    "                     1: \"NDG\",\n",
    "                     2: \"HOM\",\n",
    "                     3: \"APR\",\n",
    "                     4: \"CMP\"\n",
    "             }\n",
    "\n",
    "# Create dataframe of posts within the same category\n",
    "bodies_dataframe = {\"DEG\": [],\n",
    "                    \"NDG\": [],\n",
    "                    \"HOM\": [],\n",
    "                    \"APR\": [],\n",
    "                    \"CMP\": []\n",
    "                   }\n",
    "\n",
    "# Iterate through each entry of data, and append the body to its respected category\n",
    "for index, row in data.iterrows():\n",
    "    # Create body and label integer and string\n",
    "    body, label = str(row['body']), str(row['gold_label'])\n",
    "    if label in categ_string_list.values():\n",
    "        bodies_dataframe[label].append(body)\n",
    "# Convert to dataframe with two columns\n",
    "bodies_dataframe = pd.DataFrame(bodies_dataframe.items(), columns=['Category', 'Bodies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f0b04e-eee1-4bfd-8f5c-d8897cf8e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the vocabulary set per category \n",
    "# Flatten out list of lists for vocab set\n",
    "# https://stackoverflow.com/questions/952914/how-do-i-make-a-flat-list-out-of-a-list-of-lists\n",
    "# https://stackoverflow.com/questions/10677020/real-word-count-in-nltk\n",
    "# https://www.geeksforgeeks.org/python-statistics-stdev/\n",
    "# https://stackoverflow.com/questions/35086440/python-how-to-compute-the-top-x-most-frequently-used-words-in-an-nltk-corpus\n",
    "import itertools\n",
    "import statistics\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Create stopwords list, and add words that do not belong in the vocabulary \n",
    "Stopwords = set(upper_word.lower() for upper_word in nltk.corpus.stopwords.words('english'))\n",
    "Stopwords.add(\"http\")\n",
    "Stopwords.add(\"https\")\n",
    "Stopwords.add(\"com\")\n",
    "Stopwords.add(\"www\")\n",
    "Stopwords.add(\"r\")\n",
    "Stopwords.add(\"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560a15fc-7a0b-41cf-914c-ae818c8cc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to go through ALL documents under a specific category\n",
    "def find_vocab_set(categ):\n",
    "    # Create temporary variables and lists to store analyzed data\n",
    "    tokenized_bodies = []\n",
    "    total_tokens = 0\n",
    "    total_bodies = 0\n",
    "    size_of_doc = []\n",
    "    pronouns_num_list = []\n",
    "    total_pronouns = 0\n",
    "    most_frequent_words = []\n",
    "    # Iterate through each body in the dataframe, under the specified category\n",
    "    for body in bodies_dataframe[bodies_dataframe['Category'] == categ_string_list[categ]]['Bodies'].sum():\n",
    "        # Tokenize the body and turn it into lowercase\n",
    "        tokenized_body = tokenizer.tokenize(body.lower())\n",
    "        # Print tokenized body\n",
    "        tokenized_bodies.extend(tokenized_body)\n",
    "        # Update total tokens\n",
    "        total_tokens += len(tokenized_body)\n",
    "        # Update size of docs list\n",
    "        size_of_doc.append(len(tokenized_body))\n",
    "        # Update pronounds list\n",
    "        tmp_pronoun = extract_pronouns(body)\n",
    "        pronouns_num_list.append(tmp_pronoun)\n",
    "        # Update total pronounds count\n",
    "        total_pronouns += tmp_pronoun\n",
    "        # Update total bodies\n",
    "        total_bodies += 1\n",
    "    # Remove doubles\n",
    "    categ_dict[categ][\"vocab_set\"] = list(set(tokenized_bodies))\n",
    "    # Update vocab size\n",
    "    categ_dict[categ][\"vocab_size\"] = len(categ_dict[categ][\"vocab_set\"])\n",
    "    # Update total number of tokens\n",
    "    categ_dict[categ][\"total_tokens\"] = total_tokens\n",
    "    # Update avg tokens:\n",
    "    categ_dict[categ][\"avg_tokens\"] = total_tokens / total_bodies\n",
    "    # Update token standard deviation\n",
    "    categ_dict[categ][\"tokens_standev\"] = statistics.stdev(size_of_doc)\n",
    "    # Update avg pronouns\n",
    "    categ_dict[categ][\"avg_pronouns\"] = total_pronouns / total_bodies\n",
    "    # Update pronoun standard deviation\n",
    "    categ_dict[categ][\"prp_standev\"] = statistics.stdev(pronouns_num_list)\n",
    "    # Remove stopwords\n",
    "    bodies_without_stopwords = [word.lower() for word in tokenized_bodies if word.isalpha() and word.lower() not in Stopwords]\n",
    "    # Update frequent tokens, without stopwords\n",
    "    most_frequent_words = FreqDist(bodies_without_stopwords)\n",
    "    categ_dict[categ][\"freq_words\"] = most_frequent_words.most_common(10)\n",
    "    categ_dict[categ][\"all_words\"] = most_frequent_words.most_common(categ_dict[categ][\"vocab_size\"])\n",
    "    categ_dict[categ][\"entire_words\"] = FreqDist(tokenized_bodies).most_common(categ_dict[categ][\"vocab_size\"])\n",
    "\n",
    "# Function to extract number of pronouns in a given string\n",
    "def extract_pronouns(string):\n",
    "    # Extract pronouns using pos_tag\n",
    "    pos_list = nltk.pos_tag(word_tokenize(string))\n",
    "    number_of_pronouns = 0\n",
    "    # Run through every entry\n",
    "    for pos in pos_list:\n",
    "        if pos[1] == \"PRP\":\n",
    "            number_of_pronouns += 1\n",
    "    return number_of_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf06370c-eb89-4411-bc80-314f8799bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_vocab_set(0)\n",
    "find_vocab_set(1)\n",
    "find_vocab_set(2)\n",
    "find_vocab_set(3)\n",
    "find_vocab_set(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ead7d849-6892-4359-9ed2-26ed62d3ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print DataFrame without array values (Since they have more than one element)\n",
    "# https://stackoverflow.com/questions/31433989/return-copy-of-dictionary-excluding-specified-keys\n",
    "\n",
    "DataFrame_dict = {}\n",
    "\n",
    "def without_keys(d, keys):\n",
    "    return {k: v for k, v in d.items() if k not in keys}\n",
    "\n",
    "for key, value in categ_dict.items():\n",
    "    \"\"\"\n",
    "    print(f\"Info regarding category    {categ_string_list[key]}:\")\n",
    "    print(f\"Vocab size:                {value[\"vocab_size\"]}\")\n",
    "    print(f\"Total size:                {value[\"total_tokens\"]}\")\n",
    "    print(f\"Average tokens:            {value[\"avg_tokens\"][\"num_tokens\"]:.2f}\")\n",
    "    print(f\"Token standard dev:        {value[\"avg_tokens\"][\"standard_dev\"]:.2f}\")\n",
    "    print(f\"Average pronouns:          {value[\"avg_pronouns\"][\"num_tokens\"]:.2f}\")\n",
    "    print(f\"Pronoun standard dev:      {value[\"avg_pronouns\"][\"standard_dev\"]:.2f}\")\n",
    "    print(f\"Ten frequent words:        {value[\"freq_words\"]}\")\n",
    "    print(\"\\n\")\n",
    "    \"\"\"\n",
    "    DataFrame_dict[categ_string_list[key]] = without_keys(value, [\"vocab_set\", \"freq_words\",\"all_words\", \"entire_words\"])\n",
    "# Print dataframe using Pandas\n",
    "print(pd.DataFrame(DataFrame_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee15cf7a-781a-48ac-aa52-49da3e15c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 frequent words\n",
    "for key, value in categ_dict.items():\n",
    "    print(f\"Top ten most frequent tokens for categ {categ_string_list[key]}: \\n{value[\"freq_words\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc7406a8-af4f-485b-ade5-05391c6fe3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"WE have gathered lots of data, and can notice interesting aspects of our findings. First of all, the DEG category\\nhas the most words by far, followed, by NDG, HOM, APR, and finally CMP. However, HOM and NDG have more tokens per\\ndocument than the other categories. The lowest standard deviation for the tokens was found in the CMP and DEG \\ncategory. The most pronouns per token were found in APR and NDG. It's respective lowest standardized deviation was \\nfound in the CMP and DEG categories.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"WE have gathered lots of data, and can notice interesting aspects of our findings. First of all, the DEG category\n",
    "has the most words by far, followed, by NDG, HOM, APR, and finally CMP. However, HOM and NDG have more tokens per\n",
    "document than the other categories. The lowest standard deviation for the tokens was found in the CMP and DEG \n",
    "category. The most pronouns per token were found in APR and NDG. It's respective lowest standardized deviation was \n",
    "found in the CMP and DEG categories.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc3f21-a66d-4fd4-ab4c-b1eb3a5e8fb0",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d97f92-8168-405b-a43d-6371f36fd54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Suggest a script to draw and evaluate the zipf’s law fitting for the dataframe of each category, and\\ncomputing the corresponding R2 and adjusted R2 statistics.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Suggest a script to draw and evaluate the zipf’s law fitting for the dataframe of each category, and\n",
    "computing the corresponding R2 and adjusted R2 statistics.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "342575ca-689e-49db-92bd-293350849f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import linregress\n",
    "# https://www.geeksforgeeks.org/zipfs-law/#what-is-zipfs-law \n",
    "# https://www.geeksforgeeks.org/r-squared-vs-adjusted-r-squared-difference/\n",
    "# https://www.statology.org/r-squared-in-python/\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.reshape.html\n",
    "# https://www.statology.org/adjusted-r-squared-in-python/\n",
    "\n",
    "# Make dict to determine if function handles entire words, or all words(without stopwords)\n",
    "freq_type_dict = {0: \"entire_words\",\n",
    "                  1: \"all_words\"}\n",
    "\n",
    "# Make function to plot Zipf's law fitting and find R2 and R2 adjusted \n",
    "def zipfs_law_fitting_nltk(categ_dict, corpus_type):\n",
    "    # Print the type of data we are using, stopwords or no stopwords\n",
    "    if corpus_type == 0:\n",
    "        print(\"FOLLOWING DATA INCLUDES STOPWORDS!!!\\n\")\n",
    "    else:\n",
    "        print(\"FOLLOWING DATA EXCLUDES STOPWORDS!!!\\n\")\n",
    "    # Iterate through all entries in the category\n",
    "    for key, value in categ_dict.items():\n",
    "        # Get the tokens from the dictionary\n",
    "        entire_words = value[freq_type_dict[corpus_type]]\n",
    "        \n",
    "        #Define two lists, rank and frequency\n",
    "        rank = []\n",
    "        frequency = []\n",
    "        init = 0\n",
    "        \n",
    "        #Assign ranks based on frequencies of words\n",
    "        for freq in entire_words:\n",
    "            init+=1\n",
    "            rank.append(init)\n",
    "            frequency.append(freq[1])\n",
    "        \n",
    "        #Plot the rank and frequency\n",
    "        plt.plot(np.log(rank),np.log(frequency))\n",
    "        # Labelling the x axis\n",
    "        plt.xlabel('Rank')\n",
    "        # Labelling the y axis\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        # Providing a title to the graph\n",
    "        plt.title(f\"Zipf's law for {categ_string_list[key]}\")\n",
    "\n",
    "        # Getting r2 and adjusted r2 values\n",
    "        #initiate linear regression model\n",
    "        model = LinearRegression()\n",
    "\n",
    "        X = np.array(rank).reshape(-1, 1)\n",
    "        Y = frequency\n",
    "\n",
    "        #fit regression model\n",
    "        model.fit(X, Y)\n",
    "        \n",
    "        # Calculate R-squared of regression model\n",
    "        log_ranks = np.log(rank)\n",
    "        log_frequencies = np.log(frequency)\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(log_ranks, log_frequencies)\n",
    "        r2 = r_value ** 2\n",
    "\n",
    "        #display adjusted R-squared\n",
    "        N = len(log_frequencies)\n",
    "        r2_adjusted = 1 - ((1-r2)*(N - 1))/(N-1-1)\n",
    "        \n",
    "        # View R-squared value and adjusted\n",
    "        print(f\"R2 value for {categ_string_list[key]}         : {r2}\")\n",
    "        print(f\"R2 adjusted value for {categ_string_list[key]}: {r2_adjusted}\")\n",
    "\n",
    "\n",
    "        # show linear regression\n",
    "        log_ranks = np.log(rank)\n",
    "        log_frequencies = np.log(frequency)\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(log_ranks, log_frequencies)\n",
    "        plt.plot(log_ranks, slope * log_ranks + intercept, color='red', label=f'Fit: slope={slope:.2f}')\n",
    "        \n",
    "        # Show graph, after printing R2 and R2A \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e6b091-2158-4d28-8c0a-a0696ac1d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfs_law_fitting_nltk(categ_dict, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca2d553c-00a1-473f-a50e-206c5ff27e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We can see here that the Zipf's law fitting does apply to a large extent. Near the smaller ranks, it isn't as strong as it is \\nnear the end of the curve. However, I believe the reason behind that to be the excess use of particular tokens, specefically slurs.\\nThat could be a possible reason behind this existing, although small, disparity from the expected red line.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We can see here that the Zipf's law fitting does apply to a large extent. Near the smaller ranks, it isn't as strong as it is \n",
    "near the end of the curve. However, I believe the reason behind that to be the excess use of particular tokens, specefically slurs.\n",
    "That could be a possible reason behind this existing, although small, disparity from the expected red line.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497cecd3-c560-4ec2-8fa2-37ca4bf1e479",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccac1133-4136-4fef-8785-a6dc29c2c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfs_law_fitting_nltk(categ_dict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "400405de-1769-4c9f-80ca-1252675df041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Here we can also conclude that Zipf's law does apply to these documents. However, we must note that it isn't as strong\\nas in task 2. Since we are removing a lot of words, be ignoring the stopwords, this is a product of that. We can conclude that \\nthe presence of stopwords is quite important in applications of Zipf's Law. \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Here we can also conclude that Zipf's law does apply to these documents. However, we must note that it isn't as strong\n",
    "as in task 2. Since we are removing a lot of words, be ignoring the stopwords, this is a product of that. We can conclude that \n",
    "the presence of stopwords is quite important in applications of Zipf's Law. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409eab6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba8a90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jussi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jussi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39896835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bodies contained values that were not strings, so we had to convert them to strings\n",
    "data['body'] = data['body'].apply(lambda x: str(x) if not isinstance(x, str) else x)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bfc9267-961d-478c-937a-7b9e9c92a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing and lowering each body and then pos tagging them\n",
    "pos_tagged = []\n",
    "for body in data[\"body\"]:\n",
    "    tokens = word_tokenize(body.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    pos_tagged.append(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efa077de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the amount of each pos tag and sorting them from most frequent to least frequent\n",
    "pos_amounts = {}\n",
    "for pos in pos_tagged:\n",
    "    for word, tag in pos:\n",
    "        if tag in pos_amounts:\n",
    "            pos_amounts[tag] += 1\n",
    "        else:\n",
    "            pos_amounts[tag] = 1\n",
    "\n",
    "pos_amounts = dict(sorted(pos_amounts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(pos_amounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14831b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if Zipfs law distribution can be fitted for the frequencies of pos tags across all the bodies\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "frequencies = list(pos_amounts.values())\n",
    "ranks = range(1, len(pos_amounts) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ranks, frequencies, marker='o')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Rank/Frequency Plot')\n",
    "plt.show()\n",
    "\n",
    "log_ranks = np.log(ranks)\n",
    "log_frequencies = np.log(frequencies)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(log_ranks, log_frequencies, marker='o')\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(log_ranks, log_frequencies)\n",
    "plt.plot(log_ranks, slope * log_ranks + intercept, color='red', label=f'Fit: slope={slope:.2f}')\n",
    "\n",
    "plt.xlabel('Log(Rank)')\n",
    "plt.ylabel('Log(Frequency)')\n",
    "plt.title('Log-Log Plot')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"R-squared: {r_value ** 2}\")\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"P value: {p_value}\")\n",
    "\n",
    "print(\"There is some resemblance to Zipfs law, but not enough.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a6c28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19e0b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if Heaps law can be fitted for each category of bodies\n",
    "\n",
    "# Extracting the bodies sorted by category into a dataframe\n",
    "bodies_by_category = {}\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    # Create body and label integer and string\n",
    "    body, label = str(row['body']), str(row['gold_label'])\n",
    "    if label in bodies_by_category:\n",
    "        bodies_by_category[label].append(body)\n",
    "    else:\n",
    "        bodies_by_category[label] = [body]\n",
    "\n",
    "bodies_by_category = pd.DataFrame(bodies_by_category.items(), columns=['Category', 'Bodies'])\n",
    "\n",
    "# preprocessing the bodies and storing them in a new dataframe\n",
    "tokens_by_category = {}\n",
    "\n",
    "for index, row in bodies_by_category.iterrows():\n",
    "    bodies, label = row['Bodies'], row['Category']\n",
    "\n",
    "    for body in bodies:\n",
    "        tokens = word_tokenize(body.lower())\n",
    "        tokens = [word for word in tokens if word.isalnum()]\n",
    "        if label in tokens_by_category:\n",
    "            tokens_by_category[label].extend(tokens)\n",
    "        else:\n",
    "            tokens_by_category[label] = []\n",
    "            tokens_by_category[label] = tokens\n",
    "\n",
    "tokens_by_category = pd.DataFrame(tokens_by_category.items(), columns=['Category', 'Tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a370e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPointsCalculator(category):\n",
    "    tokens = tokens_by_category[tokens_by_category['Category'] == category]['Tokens'].sum()\n",
    "    points = []\n",
    "    unique_tokens = []\n",
    "\n",
    "    for i in range(1, len(tokens)):\n",
    "        if tokens[i] not in unique_tokens:\n",
    "            unique_tokens.append(tokens[i])\n",
    "        points.append((i, len(unique_tokens)))\n",
    "\n",
    "    return points\n",
    "\n",
    "uniqueTokensByTotalTokens = {\n",
    "    'DEG': DataPointsCalculator('DEG'),\n",
    "    'NDG': DataPointsCalculator('NDG'),\n",
    "    'HOM': DataPointsCalculator('HOM'),\n",
    "    'APR': DataPointsCalculator('APR'),\n",
    "    'CMP': DataPointsCalculator('CMP')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea59f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def HeapsLaw(N, k, b):\n",
    "    return k * N ** b\n",
    "\n",
    "heaps_fitting_results = {}\n",
    "\n",
    "for row, points in uniqueTokensByTotalTokens.items():\n",
    "    x_values, y_values = zip(*points)\n",
    "    x_values = np.array(x_values)\n",
    "    y_values = np.array(y_values)\n",
    "\n",
    "    popt, _ = curve_fit(HeapsLaw, x_values, y_values, maxfev=10000)\n",
    "    k, b = popt\n",
    "    \n",
    "    y_pred = HeapsLaw(x_values, k, b)\n",
    "    r2 = r2_score(y_values, y_pred)\n",
    "    heaps_fitting_results[row] = {'k': k, 'beta': b, 'R2': r2}\n",
    "    \n",
    "    plt.plot(x_values, y_values, label='Data')\n",
    "    plt.plot(x_values, y_pred, label=f'R^2={r2:.2f}, K={k:.2f}, beta={b:.2f}', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Total Tokens')\n",
    "    plt.ylabel('Unique Tokens')\n",
    "    plt.title(f'Heaps law fit for {row} category')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"For {row} category with k={k:.2f} and b={b:.2f}, the R^2 value is {r2:.2f}\")\n",
    "    if r2 > 0.9:\n",
    "        print(\"Heaps law fit is good\")\n",
    "    else:\n",
    "        print(\"Heaps law fit is not good\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9dcaf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0d11e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary for each category\n",
    "\n",
    "vocabularies_by_category = {}\n",
    "\n",
    "for index, row in tokens_by_category.iterrows():\n",
    "    categ = row['Category']\n",
    "    tokens = row['Tokens']\n",
    "    vocabulary = set(tokens)\n",
    "    vocabularies_by_category[categ] = vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "580240ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['DEG', 'NDG', 'HOM', 'APR', 'CMP']\n",
    "\n",
    "# Jaccard similarity between the vocabularies of categories\n",
    "jaccard_similarities = {}\n",
    "for i in range(0, len(categories)):\n",
    "    for j in range(i + 1, len(categories)):\n",
    "        category1 = categories[i]\n",
    "        category2 = categories[j]\n",
    "        vocabulary1 = vocabularies_by_category[category1]\n",
    "        vocabulary2 = vocabularies_by_category[category2]\n",
    "        jaccard_similarity = len(vocabulary1.intersection(vocabulary2)) / len(vocabulary1.union(vocabulary2))\n",
    "        jaccard_similarities[(category1, category2)] = jaccard_similarity\n",
    "\n",
    "print(\"Jaccard similarity between the vocabularies of each category pair:\")\n",
    "for categories, similarity in jaccard_similarities.items():\n",
    "    print(f\"{categories[0]} and {categories[1]}: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccdea9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b01f9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each body in tokenized form by category\n",
    "token_bodies_by_category = {}\n",
    "\n",
    "for index, row in bodies_by_category.iterrows():\n",
    "    categ = row['Category']\n",
    "    bodies = row['Bodies']\n",
    "    for body in bodies:\n",
    "\n",
    "        tokens = word_tokenize(body.lower())\n",
    "        tokens = [word for word in tokens if word.isalnum()]\n",
    "\n",
    "        if categ in token_bodies_by_category:\n",
    "            token_bodies_by_category[categ].append(tokens)\n",
    "        else:\n",
    "            token_bodies_by_category[categ] = []\n",
    "            token_bodies_by_category[categ].append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50b601a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tagging\n",
    "pos_token_bodies_by_category = {}\n",
    "\n",
    "for row, token_bodies in token_bodies_by_category.items():\n",
    "    pos_token_bodies_by_category[row] = []\n",
    "    for token_body in token_bodies:\n",
    "        pos_token_bodies_by_category[row].append(nltk.pos_tag(token_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00ad421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the pos tags for tokens around the modal verb with window size of 2\n",
    "\n",
    "pos_tags_around_MD = {}\n",
    "\n",
    "for row, pos_token_bodies in pos_token_bodies_by_category.items():\n",
    "    pos_tags_around_MD[row] = []\n",
    "    for body in pos_token_bodies:\n",
    "        for i in range(0, len(body)):\n",
    "            if body[i][1] == 'MD':\n",
    "                start = max(i - 2, 0)\n",
    "                end = min(i + 3, len(body))\n",
    "                window = [token[1] for j, token in enumerate(body[start:end]) if (start + j) != i]\n",
    "                pos_tags_around_MD[row].append(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57fdff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the frequencies for all the pos tags around the MD by category\n",
    "# and saving them in a dataframe\n",
    "pos_tags_around_MD_frequencies = {}\n",
    "\n",
    "for row, pos_token_list in pos_tags_around_MD.items():\n",
    "    pos_tags_around_MD_frequencies[row] = {}\n",
    "    for pos_tag_list in pos_token_list:\n",
    "        for pos_tag in pos_tag_list:\n",
    "            if pos_tag in pos_tags_around_MD_frequencies[row]:\n",
    "                pos_tags_around_MD_frequencies[row][pos_tag] += 1\n",
    "            else:\n",
    "                pos_tags_around_MD_frequencies[row][pos_tag] = 1\n",
    "\n",
    "# Cleanup for the dataframe\n",
    "pos_tags_around_MD_frequencies = pd.DataFrame(pos_tags_around_MD_frequencies)\n",
    "pos_tags_around_MD_frequencies = pos_tags_around_MD_frequencies.dropna(axis=1, how='all')\n",
    "pos_tags_around_MD_frequencies = pos_tags_around_MD_frequencies.fillna(0)\n",
    "pos_tags_around_MD_frequencies = pos_tags_around_MD_frequencies.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f132f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_tags_around_MD_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c77da41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the frequencies of pos tags around the MD with window of 2 for each category\n",
    "for row in pos_tags_around_MD_frequencies.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    sorted_values = pos_tags_around_MD_frequencies[row].sort_values(ascending=False)\n",
    "    \n",
    "    sorted_values.plot(kind='bar')\n",
    "    plt.title(f'POS tag frequencies for tokens in window of 2 around modal verbs.\\n For {row} category')\n",
    "    plt.xlabel('POS Tag')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37f6d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between pos tag frequencies of each pair of categories\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "category_vectors = pos_tags_around_MD_frequencies.T\n",
    "\n",
    "cosine_sim_matrix = pd.DataFrame(cosine_similarity(category_vectors), index=category_vectors.index, \n",
    "                                 columns=category_vectors.index)\n",
    "print(\"Cosine similarity matrix between the frequencies of pos tags in a window of 2 around modal verbs\")\n",
    "print(\"for each category pair:\")\n",
    "print(cosine_sim_matrix)\n",
    "\n",
    "print(\"\\nAll the categories are quite similar to each other according to cosine similarity.\")\n",
    "print(\"This way of comparing similarity does not take into account the sample sizes.\")\n",
    "print(\"Small sample size can lead to misleading results.\")\n",
    "print(\"Smallest sample is CMP with 133, and largest is NDG with 33729, \")\n",
    "print(\"so the similarity between NDG and CMP is not as reliable as the similarity between NDG and other large set like DEG.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea164003-9620-4061-8b28-9937bc3e6b0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd84d84-8626-4cd5-930c-766ecd4ca8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use Vader sentiment analysis package https://github.com/cjhutto/vaderSentiment to computer the\n",
    "overall sentiment of each post, then draw a 10-bin histogram showing the proportion of posts whose\n",
    "overall sentiment score falls in a given bin. This yields four histogram distributions (one histogram\n",
    "distribution for each dataframe). Next, use the standard Euclidean histogram distance to compute the\n",
    "similarity between every pair of categories with respect to sentiment scores.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d28788-7455-4adf-a211-8814215b2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sentece and initialize analyzer \n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Creat dictionary to store sentiment values as a element of a list, as a value\n",
    "sentiment_dict  = {  \"DEG\" : [],\n",
    "                     \"NDG\" : [],\n",
    "                     \"HOM\" : [],\n",
    "                     \"APR\" : [],\n",
    "                     \"CMP\" : []\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a67ab5-9f59-4d4a-8e92-632c9bec3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vocab_sent(categ):\n",
    "    for index, row in data.iterrows():\n",
    "        # Create body and label integer and string\n",
    "        body, label = str(row['body']), str(row['gold_label'])\n",
    "        # Check if the label is deg, ndg, hom, apr, or cmp\n",
    "        if label == categ_string_list[categ]:\n",
    "            # Find sentiment score and append to respected category list\n",
    "            vs = analyzer.polarity_scores(str(row['body']))\n",
    "            sentiment_dict[categ_string_list[categ]].append(vs['compound'])\n",
    "find_vocab_sent(0)\n",
    "find_vocab_sent(1)\n",
    "find_vocab_sent(2)\n",
    "find_vocab_sent(3)\n",
    "find_vocab_sent(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a443644-bbb0-4485-b3f1-99e7309239af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "for key, value in sentiment_dict.items():\n",
    "    print(f\"Average sentiment for {key}: {sum(value) / len(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d08a4-1409-424a-bcd8-744b20c6a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms\n",
    "# https://www.geeksforgeeks.org/plotting-histogram-in-python-using-matplotlib/\n",
    "# https://stackoverflow.com/questions/20128898/get-data-points-from-a-histogram-in-python\n",
    "import seaborn as sns\n",
    "\n",
    "# Iterate through each entry of sentiment dictionary\n",
    "for key, value in sentiment_dict.items():\n",
    "    # Plot a histogram of sentiment values, with 10 bins\n",
    "    sns.histplot(value, bins=10, kde=True, color='lightgreen', edgecolor='red')\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Sentiment Histogram for category {key}')\n",
    "     \n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c61a68-d129-48e2-bfdc-a89a6eae2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now iterate through each pair of categories, and find euclidian distance\n",
    "# https://www.geeksforgeeks.org/calculate-the-euclidean-distance-using-numpy/\n",
    "# https://stackoverflow.com/questions/9314576/calculate-distance-between-two-vectors-of-different-length\n",
    "# First, we must fill the shorter lists with 0s\n",
    "\n",
    "# Find largest list size\n",
    "N = 0\n",
    "for key, value in sentiment_dict.items():\n",
    "    # If the new category is larger than current largest, update N\n",
    "    if len(value) > N:\n",
    "        N = len(value)\n",
    "\n",
    "# Now fill the smaller lists with 0s\n",
    "# https://stackoverflow.com/questions/43336837/making-equal-size-lists-in-python\n",
    "for key, value in sentiment_dict.items(): \n",
    "    # If the category is smaller than the largest one,\n",
    "    if len(value) < N:\n",
    "        # Fill the end of the list with 0s\n",
    "        value.extend([0] * (N-len(value)))\n",
    "\n",
    "# Make list of all categories \n",
    "categories = ['DEG', 'NDG', 'HOM', 'APR', 'CMP']\n",
    "# Iterate through each pair of categories\n",
    "for i in range(0, len(sentiment_dict.keys())):\n",
    "    for j in range(i + 1, len(sentiment_dict.keys())):\n",
    "        # Assign categ 1 and categ 2\n",
    "        categ_1 = categories[i]\n",
    "        categ_2 = categories[j]\n",
    "        # Assigns sentiment list 1 and 2\n",
    "        sent_1 = sentiment_dict[categ_1]\n",
    "        sent_2 = sentiment_dict[categ_2]\n",
    "        # Find the Euclidian distance between these two lists\n",
    "        dist = np.linalg.norm(np.array(sent_1) - np.array(sent_2))\n",
    "        # Neatly print the distance to a degree of five decimal points\n",
    "        print(f\"Euclidian distance between {categ_1} and {categ_2}: {dist:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29628297",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca505db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stopwords from the tokens since we dont need those for empath categorization\n",
    "tokens_by_category_stopwords_removed = {}\n",
    "\n",
    "for index, row in tokens_by_category.iterrows():\n",
    "    category = str(row['Category'])\n",
    "    tokens = row['Tokens']\n",
    "\n",
    "    tokens_by_category_stopwords_removed[category] = [word for word in tokens if word.lower() not in Stopwords]\n",
    "\n",
    "# For some reason there was annoying NaN key in the dictionary, so this removes it\n",
    "for key in tokens_by_category_stopwords_removed.keys():\n",
    "    if pd.isna(key):\n",
    "        tokens_by_category_stopwords_removed.pop(key)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b283a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vocabulary for each category for the embedding vectors\n",
    "vocabularies_by_category_stopwords_removed = {}\n",
    "\n",
    "for category, tokens in tokens_by_category_stopwords_removed.items():\n",
    "    vocabularies_by_category_stopwords_removed[category] = set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the embedding vectors for each category with Empath\n",
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "embedding_vectors_by_category = {}\n",
    "\n",
    "for category, vocabulary in vocabularies_by_category_stopwords_removed.items():\n",
    "    vocabulary_string = ' '.join(vocabulary)\n",
    "    embedding_vector = lexicon.analyze(vocabulary_string, normalize=True)\n",
    "    embedding_vectors_by_category[category] = embedding_vector\n",
    "\n",
    "# annoying NaN key came back again\n",
    "if 'nan' in embedding_vectors_by_category:\n",
    "    del embedding_vectors_by_category['nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity between the embedding vectors of each pair of categories\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "categories = ['DEG', 'NDG', 'HOM', 'APR', 'CMP']\n",
    "\n",
    "empath_cosine_similarities = {}\n",
    "\n",
    "for i in range(0, len(categories)):\n",
    "    for j in range(i + 1, len(categories)):\n",
    "        category1 = categories[i]\n",
    "        category2 = categories[j]\n",
    "\n",
    "        vector1 = list(embedding_vectors_by_category[category1].values())\n",
    "        vector2 = list(embedding_vectors_by_category[category2].values())\n",
    "        cosine_sim = cosine_similarity([vector1], [vector2])[0][0]\n",
    "        empath_cosine_similarities[(category1, category2)] = cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine similarities between the embedding vectors of each pair of categories with Empath:\")\n",
    "for pair in empath_cosine_similarities:\n",
    "    print(f\"{pair[0]} and {pair[1]}: {empath_cosine_similarities[pair]:.2f}\")\n",
    "print(\"This way comparing the categories fails to capture where the categories are similar and where they are not.\")\n",
    "print(\"The tool gives a score for many different empath categories,\")\n",
    "print(\"and when the comparison between categories is combined into one number, information gets lost.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab1fec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Doc2Vec\n",
    "\n",
    "# # Load the model\n",
    "# model_path = 'models/doc2vec.bin'\n",
    "# model = Doc2Vec.load(model_path)\n",
    "\n",
    "# # Calculate embeddings for each category\n",
    "# category_embeddings = {}\n",
    "\n",
    "# for category, tokens in tokens_by_category_stopwords_removed.items():\n",
    "#     token_vectors = [model.infer_vector(token) for token in tokens if token in model.wv]\n",
    "#     if token_vectors:\n",
    "#         category_embeddings[category] = np.mean(token_vectors, axis=0)\n",
    "#     else:\n",
    "#         category_embeddings[category] = np.zeros(model.vector_size)\n",
    "\n",
    "# # Calculate the cosine similarities\n",
    "# similarity_df = pd.DataFrame(index=category_embeddings.keys(), columns=category_embeddings.keys())\n",
    "\n",
    "# for category1, embedding1 in category_embeddings.items():\n",
    "#     for category2, embedding2 in category_embeddings.items():\n",
    "#         similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "#         similarity_df.loc[category1, category2] = similarity\n",
    "\n",
    "# print(similarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527689b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in bodies_by_category.iterrows():\n",
    "    bodies, label = row['Bodies'], row['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DistilBERT to compute the embeddings of each post in a category and average them to get the category embedding\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import random\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You can use your GPU.\")\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print number of bodies in each category\n",
    "for category, bodies in bodies_by_category.iterrows():\n",
    "    print(f'{category} category number of bodies: {len(bodies[1])}')\n",
    "\n",
    "# Function to get the embeddings of a sentence\n",
    "def get_embeddings(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.cpu()\n",
    "\n",
    "# Calculate the embeddings for each post in a category and average them to get the category embedding\n",
    "category_embeddings = {}\n",
    "\n",
    "for index, row in bodies_by_category.iterrows():\n",
    "    \n",
    "    bodies, category = row['Bodies'], row['Category']\n",
    "    bodies = random.sample(bodies, 100)  # Sample 100 random posts from each category\n",
    "    \n",
    "    embeddings_sum = torch.zeros(model.config.dim)\n",
    "    num_valid_bodies = 0  # Keep track of valid (non-empty) posts\n",
    "\n",
    "    for body in bodies:\n",
    "        if body.strip():  # Check if the body has content\n",
    "            body_embedding = get_embeddings(body)\n",
    "            embeddings_sum += body_embedding\n",
    "            num_valid_bodies += 1\n",
    "\n",
    "    if num_valid_bodies > 0:\n",
    "        category_embeddings[category] = embeddings_sum / num_valid_bodies\n",
    "    else:\n",
    "        category_embeddings[category] = torch.zeros(model.config.dim)\n",
    "\n",
    "# Print the average embeddings for each category\n",
    "for category, embedding in category_embeddings.items():\n",
    "    # get the category name from the index\n",
    "    print(f'Category name: {category}')\n",
    "    print(f'  Mean: {embedding.mean().item():.4f}')\n",
    "    print(f'  Min: {embedding.min().item():.4f}')\n",
    "    print(f'  Max: {embedding.max().item():.4f}')\n",
    "    \n",
    "# Calculate the cosine similarities of each pair of categories\n",
    "cosine_similarities = {}\n",
    "\n",
    "for category1, embedding1 in category_embeddings.items():\n",
    "    for category2, embedding2 in category_embeddings.items():\n",
    "        similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2, dim=0).item()\n",
    "        cosine_similarities[(category1, category2)] = similarity\n",
    "        \n",
    "# Print the cosine similarities\n",
    "for categories, similarity in cosine_similarities.items():\n",
    "    print(f'{categories[0]} and {categories[1]}: {similarity:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9a2d4-bfe6-4648-aee8-37e041bf19fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7fefe-f9ac-431d-b892-c1f3b899f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We want to evaluate the various categories in terms of linguistic quality of the posts. For this purpose,\n",
    "suggest a script to identify the number of unmatched tokens in each dataframe, for instance by seeking\n",
    "whether the token has an entry in Wordnet. You may also inspire by some existing implementation such\n",
    "as Spelling Correction Of The Text Data In Natural Language Processing | by Nutan | Medium. Estimate\n",
    "the number of spelling errors in each category and its proportion with respect to the total number of\n",
    "posts in the category.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f30cdb-388c-45f6-82b7-c2059ad5c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to run through all bodies of a specified category\n",
    "# Create dictionary to hold how many errors are in each category\n",
    "# https://medium.com/@nutanbhogendrasharma/spelling-correction-of-the-text-data-in-natural-language-processing-e9848407cf3b\n",
    "\n",
    "# Import spell checker and initiate it\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Make a dictionary to track errors in each category\n",
    "error_dict ={\"DEG\": 0,\n",
    "             \"NDG\": 0,\n",
    "             \"HOM\": 0,\n",
    "             \"APR\": 0,\n",
    "             \"CMP\": 0\n",
    "             }\n",
    "\n",
    "# Make a dictionary to track the number of posts in each dictionary, this will be useful later\n",
    "posts_dict ={\"DEG\": 0,\n",
    "             \"NDG\": 0,\n",
    "             \"HOM\": 0,\n",
    "             \"APR\": 0,\n",
    "             \"CMP\": 0\n",
    "             }\n",
    "# Make function to analyze posts of quality under specific category\n",
    "def quality_analysis(categ):\n",
    "    # Iterate each index\n",
    "    for index, row in data.iterrows():\n",
    "        # Create body and label integer and string\n",
    "        body, label = str(row['body']), str(row['gold_label'])\n",
    "        # If the label is the one we are looking for, continue\n",
    "        if label == categ_string_list[categ]:\n",
    "            # Print unknown, mispelt words\n",
    "            # Remove punctuation as it seemingly affects the spellchecker\n",
    "            #print(spell.unknown(body.split()))\n",
    "            potential_error = spell.unknown(tokenizer.tokenize(body))\n",
    "            # Update dictionary values\n",
    "            error_dict[categ_string_list[categ]] += len(potential_error)\n",
    "            posts_dict[categ_string_list[categ]] += 1\n",
    "quality_analysis(0)\n",
    "quality_analysis(1)\n",
    "quality_analysis(2)\n",
    "quality_analysis(3)\n",
    "quality_analysis(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57225280-bd0f-4da3-a2a1-937952f0732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the proportion of errors per post for each category\n",
    "for key, value in error_dict.items():\n",
    "    print(f\"                 Potential errors in categ {key}: {value}\")\n",
    "    print(f\"Proportion of errors per post for category {key}: {value/posts_dict[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762bf64-4194-468e-89d1-444f5d2d62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here we can see the cateogry with the most potential spelling erros per post is CMP, and the category with the \n",
    "least potential spelling erros per post is DEG. This data may not be the best representation since there are far more\n",
    "posts in DEG than CMP.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef7020",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2af35-25d8-4b98-aef7-5992f853d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "from nltk.metrics import edit_distance\n",
    "import os\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "already_found = {}\n",
    "\n",
    "# Function to find the closest correction and calculate edit distance\n",
    "def find_corrections_and_distances(categ):\n",
    "    corrections = []\n",
    "    distances = []\n",
    "\n",
    "    tokens = tokens_by_category_stopwords_removed[categ_string_list[categ]]\n",
    "    \n",
    "    misspelled = spell.unknown(tokens)\n",
    "    \n",
    "    for word in misspelled:\n",
    "        if word in already_found.keys():\n",
    "            correction = already_found[word]\n",
    "        else:\n",
    "            correction = spell.correction(word)\n",
    "            if correction:\n",
    "                already_found[word] = correction\n",
    "        if correction:\n",
    "            corrections.append((word, correction))\n",
    "            distances.append(edit_distance(word, correction))\n",
    "\n",
    "    print(f\"Category: {categ_string_list[categ]}\")\n",
    "    \n",
    "    return corrections, distances\n",
    "\n",
    "# Dictionary to store corrections and distances for each category\n",
    "corrections_dict = {}\n",
    "distances_dict = {}\n",
    "\n",
    "# Get corrections and distances for each category from files if they exist\n",
    "if os.path.exists('corrections.csv') and os.path.exists('distances.csv'):\n",
    "    with open('corrections.csv', 'r', encoding='utf-8') as f:\n",
    "        current_category = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if ',' not in line:\n",
    "                current_category = line\n",
    "                corrections_dict[current_category] = []\n",
    "            else:\n",
    "                correction = line.split(',')\n",
    "                corrections_dict[current_category].append((correction[0], correction[1]))\n",
    "    with open('distances.csv', 'r', encoding='utf-8') as f:\n",
    "        current_category = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if any(char.isalpha() for char in line):\n",
    "                current_category = line\n",
    "                distances_dict[current_category] = []\n",
    "            else:\n",
    "                distances_dict[current_category].append(int(line))\n",
    "else:\n",
    "    # Find corrections and distances for each category\n",
    "    for i in range(len(categ_string_list)-1):\n",
    "        corrections, distances = find_corrections_and_distances(i)\n",
    "        corrections_dict[categ_string_list[i]] = corrections\n",
    "        distances_dict[categ_string_list[i]] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a88608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the corrections and distances to a file\n",
    "if not os.path.exists('corrections.csv') and not os.path.exists('distances.csv'):\n",
    "    with open('corrections.csv', 'w', encoding='utf-8') as f:\n",
    "        for category, corrections in corrections_dict.items():\n",
    "            f.write(f'{category}\\n')\n",
    "            for correction in corrections:\n",
    "                f.write(f'{correction[0]},{correction[1]}\\n')\n",
    "    with open('distances.csv', 'w', encoding='utf-8') as f:\n",
    "        for category, distances in distances_dict.items():\n",
    "            f.write(f'{category}\\n')\n",
    "            for distance in distances:\n",
    "                f.write(f'{distance}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be613c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze edit distances by category by plotting a histogram\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for category, distances in distances_dict.items():\n",
    "    p = sns.histplot(distances, bins=range(0, max(distances) + 2), kde=False)\n",
    "    \n",
    "    p.set_xticks(range(0, max(distances) + 2))\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Edit Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Edit Distance Histogram for category {category}')\n",
    "    \n",
    "    # Add gridlines to improve readability\n",
    "    plt.grid(visible=True, which='major', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Save corrections to txt file\n",
    "with open('corrections.txt', 'w', encoding='utf-8') as f:\n",
    "    for category, corrections in corrections_dict.items():\n",
    "        f.write(f'Category: {category}\\n')\n",
    "        for correction in corrections:\n",
    "            f.write(f'{correction[0]} -> {correction[1]}\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "# Print table of results\n",
    "print(\"Edit distances by category:\")\n",
    "for category, distances in distances_dict.items():\n",
    "    print(f\"{category}:\")\n",
    "    print(f\"Mean: {np.mean(distances):.2f}\")\n",
    "    print(f\"Median: {np.median(distances)}\")\n",
    "    print(f\"Standard deviation: {np.std(distances):.2f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
